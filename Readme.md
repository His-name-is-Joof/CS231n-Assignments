# cs231n solutions
These contain my solutions for [Stanfords CS231n: Convolutional Neural Networks for Visual Recognition](http://vision.stanford.edu/teaching/cs231n/)

To run assignment3, you must first download the data usng the scripts in cs231/datasets/

## [Assignment 1](assignment1.md)
### Q1: k-Nearest Neighbor classifier (30 points) [COMPLETE]

The IPython Notebook **knn.ipynb** will walk you through implementing the kNN classifier.

### Q2: Training a Support Vector Machine (30 points) [COMPLETE]

The IPython Notebook **svm.ipynb** will walk you through implementing the SVM classifier.

### Q3: Implement a Softmax classifier (30 points)[COMPLETE]

The IPython Notebook **softmax.ipynb** will walk you through implementing the Softmax classifier.

### Q4: Higher Level Representations: Image Features (10 points) [COMPLETE]

The IPython Notebook **features.ipynb** will walk you through this exercise, in which you will examine the improvements gained by using higher-level representations as opposed to using raw pixel values.

## [Assignment 2](assignment2.md)
### Q1: Fully-connected Neural Network (30 points)[COMPLETE]

The IPython notebook FullyConnectedNets.ipynb will introduce you to our modular layer design, and then use those layers to implement fully-connected networks of arbitrary depth. To optimize these models you will implement several popular update rules.

### Q2: Batch Normalization (30 points)[COMPLETE]

In the IPython notebook BatchNormalization.ipynb you will implement batch normalization, and use it to train deep fully-connected networks.

### Q3: Dropout (10 points)[COMPLETE]

The IPython notebook Dropout.ipynb will help you implement Dropout and explore its effects on model generalization.

### Q4: ConvNet on CIFAR-10 (30 points)[COMPLETE]

In the IPython Notebook ConvolutionalNetworks.ipynb you will implement several new layers that are commonly used in convolutional networks. You will train a (shallow) convolutional network on CIFAR-10, and it will then be up to you to train the best network that you can.

## [Assignment 3](assignement3.md)
### Q1: Image Captioning with Vanilla RNNs (40 points)[COMPLETE]
The IPython notebook `RNN_Captioning.ipynb` will walk you through the
implementation of an image captioning system on MS-COCO using vanilla recurrent
networks.

### Q2: Image Captioning with LSTMs (35 points)[COMPLETE]
The IPython notebook `LSTM_Captioning.ipynb` will walk you through the
implementation of Long-Short Term Memory (LSTM) RNNs, and apply them to image
captioning on MS-COCO.

### Q3: Image Gradients: Saliency maps and Fooling Images (10 points)[IN-PROGESS]
The IPython notebook `ImageGradients.ipynb` will introduce the TinyImageNet
dataset. You will use a pretrained model on this dataset to compute gradients
with respect to the image, and use them to produce saliency maps and fooling
images.

### Q4: Image Generation: Classes, Inversion, DeepDream (15 points)[IN-PROGRESS]
In the IPython notebook `ImageGeneration.ipynb` you will use the pretrained
TinyImageNet model to generate images. In particular you will generate
class visualizations and implement feature inversion and DeepDream.

